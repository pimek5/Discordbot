{
	"version": "2.0.0",
	"tasks": [
		{
			"label": "format-scraper",
			"type": "shell",
			"command": "python -m pyflakes creator/creator_scraper.py",
			"args": [],
			"isBackground": false,
			"problemMatcher": []
		},
		{
			"label": "inject-divineskins-helpers",
			"type": "shell",
			"command": "$p='creator/creator_scraper.py'; $c = Get-Content $p -Raw; $c = $c -replace [regex]::Escape(\"html = await response.text()`r`n                    soup = BeautifulSoup(html, 'html.parser')`r`n                    data = {'username': username, 'platform': 'divineskins', 'profile_url': url}`r`n                    # TODO: Update selectors`r`n                    rank = soup.find('div', class_='rank')`r`n                    if rank:`r`n                        data['rank'] = rank.get_text(strip=True)`r`n                    skins = soup.find('div', class_='skins-count')`r`n                    if skins:`r`n                        data['total_mods'] = self._parse_number(skins.get_text(strip=True))`r`n                    dls = soup.find('div', class_='downloads-count')`r`n                    if dls:`r`n                        data['total_downloads'] = self._parse_number(dls.get_text(strip=True))\"), \"html = await response.text()`r`n                    data = {'username': username, 'platform': 'divineskins', 'profile_url': url}`r`n                    next_data = self._extract_next_data(html)`r`n                    if next_data:`r`n                        parsed = self._parse_next_profile(next_data)`r`n                        data.update(parsed)`r`n                        logger.info(\\\"✅ Divine Skins profile fetched via __NEXT_DATA__: %s\\\", username)`r`n                        return data`r`n                    soup = BeautifulSoup(html, 'html.parser')`r`n                    rank = soup.find('div', class_='rank')`r`n                    if rank:`r`n                        data['rank'] = rank.get_text(strip=True)`r`n                    logger.info(\\\"✅ Divine Skins profile fetched (minimal): %s\\\", username)`r`n                    return data\"; $c = $c -replace [regex]::Escape(\"# Fallback to HTML`r`n            url = f\\\"https://divineskins.gg/{username}/skins\\\"`r`n            async with aiohttp.ClientSession() as session:`r`n                async with session.get(url) as response:`r`n                    if response.status != 200:`r`n                        logger.error(\\\"❌ Failed to fetch skins: %s\\\", url)`r`n                        return []`r`n                    html = await response.text()`r`n                    soup = BeautifulSoup(html, 'html.parser')`r`n                    skins = []`r`n                    cards = soup.find_all('div', class_='skin-card')`r`n                    for card in cards:`r`n                        try:`r`n                            link = card.find('a', href=True)`r`n                            if not link:`r`n                                continue`r`n                            skin_url = link['href']`r`n                            if not skin_url.startswith('http'):`r`n                                skin_url = f\\\"https://divineskins.gg{skin_url}\\\"`r`n                            skin_id = skin_url.split('/')[-1]`r`n                            skin_name = link.get_text(strip=True)`r`n                            time_el = card.find('time')`r`n                            updated_at = time_el.get('datetime', '') if time_el else ''`r`n                            skins.append({'id': skin_id, 'name': skin_name, 'url': skin_url, 'updated_at': updated_at})`r`n                        except Exception as e:`r`n                            logger.error(\\\"❌ Error parsing skin card: %s\\\", e)`r`n                            continue`r`n                    logger.info(\\\"✅ Found %s skins for %s on Divine Skins\\\", len(skins), username)`r`n                    return skins\"), \"# Fallback to HTML - parse All Works from profile page`r`n            url = f\\\"https://divineskins.gg/{username}\\\"`r`n            async with aiohttp.ClientSession() as session:`r`n                async with session.get(url) as response:`r`n                    if response.status != 200:`r`n                        logger.error(\\\"❌ Failed to fetch profile page: %s\\\", url)`r`n                        return []`r`n                    html = await response.text()`r`n                    skins = []`r`n                    next_data = self._extract_next_data(html)`r`n                    if next_data:`r`n                        skins = self._parse_next_works(next_data)`r`n                        logger.info(\\\"✅ Parsed %s works from __NEXT_DATA__ for %s\\\", len(skins), username)`r`n                        return skins`r`n                    soup = BeautifulSoup(html, 'html.parser')`r`n                    for a in soup.find_all('a', href=True):`r`n                        $href = $a.href`r`n                    \" ); Set-Content $p $c; Write-Host 'Patched';",
			"args": [],
			"isBackground": false,
			"problemMatcher": []
		},
		{
			"label": "patch-divineskins-profile",
			"type": "shell",
			"command": "$p='creator/creator_scraper.py'; $old = @'\n                    html = await response.text()\n                    soup = BeautifulSoup(html, 'html.parser')\n                    data = {'username': username, 'platform': 'divineskins', 'profile_url': url}\n                    # TODO: Update selectors\n                    rank = soup.find('div', class_='rank')\n                    if rank:\n                        data['rank'] = rank.get_text(strip=True)\n                    skins = soup.find('div', class_='skins-count')\n                    if skins:\n                        data['total_mods'] = self._parse_number(skins.get_text(strip=True))\n                    dls = soup.find('div', class_='downloads-count')\n                    if dls:\n                        data['total_downloads'] = self._parse_number(dls.get_text(strip=True))\n'@; $new = @'\n                    html = await response.text()\n                    data = {'username': username, 'platform': 'divineskins', 'profile_url': url}\n                    next_data = self._extract_next_data(html)\n                    if next_data:\n                        parsed = self._parse_next_profile(next_data)\n                        data.update(parsed)\n                        logger.info(\"✅ Divine Skins profile fetched via __NEXT_DATA__: %s\", username)\n                        return data\n                    soup = BeautifulSoup(html, 'html.parser')\n                    rank = soup.find('div', class_='rank')\n                    if rank:\n                        data['rank'] = rank.get_text(strip=True)\n                    logger.info(\"✅ Divine Skins profile fetched (minimal): %s\", username)\n                    return data\n'@; (Get-Content $p -Raw).Replace($old, $new) | Set-Content $p -NoNewline; Write-Host 'Profile block patched'",
			"args": [],
			"isBackground": false,
			"problemMatcher": []
		},
		{
			"label": "patch-divineskins-skins",
			"type": "shell",
			"command": "$p='creator/creator_scraper.py'; $old = @'\n            # Fallback to HTML\n            url = f\"https://divineskins.gg/{username}/skins\"\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    if response.status != 200:\n                        logger.error(\"❌ Failed to fetch skins: %s\", url)\n                        return []\n                    html = await response.text()\n                    soup = BeautifulSoup(html, 'html.parser')\n                    skins = []\n                    cards = soup.find_all('div', class_='skin-card')\n                    for card in cards:\n                        try:\n                            link = card.find('a', href=True)\n                            if not link:\n                                continue\n                            skin_url = link['href']\n                            if not skin_url.startswith('http'):\n                                skin_url = f\"https://divineskins.gg{skin_url}\"\n                            skin_id = skin_url.split('/')[-1]\n                            skin_name = link.get_text(strip=True)\n                            time_el = card.find('time')\n                            updated_at = time_el.get('datetime', '') if time_el else ''\n                            skins.append({'id': skin_id, 'name': skin_name, 'url': skin_url, 'updated_at': updated_at})\n                        except Exception as e:\n                            logger.error(\"❌ Error parsing skin card: %s\", e)\n                            continue\n                    logger.info(\"✅ Found %s skins for %s on Divine Skins\", len(skins), username)\n                    return skins\n'@; $new = @'\n            # Fallback to HTML - parse All Works from profile page\n            url = f\"https://divineskins.gg/{username}\"\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    if response.status != 200:\n                        logger.error(\"❌ Failed to fetch profile page: %s\", url)\n                        return []\n                    html = await response.text()\n                    skins = []\n                    next_data = self._extract_next_data(html)\n                    if next_data:\n                        skins = self._parse_next_works(next_data)\n                        logger.info(\"✅ Parsed %s works from __NEXT_DATA__ for %s\", len(skins), username)\n                        return skins\n                    soup = BeautifulSoup(html, 'html.parser')\n                    # Best-effort fallback: collect links that look like mod pages\n                    for a in soup.find_all('a', href=True):\n                        href = a['href']\n                        if '/mods/' in href or '/explore-mods/' in href:\n                            skin_url = href if href.startswith('http') else f\"https://divineskins.gg{href}\"\n                            skin_id = skin_url.rstrip('/').split('/')[-1]\n                            skin_name = a.get_text(strip=True) or 'Untitled'\n                            skins.append({'id': skin_id, 'name': skin_name, 'url': skin_url, 'updated_at': ''})\n                    # Deduplicate by URL\n                    $dict = @{}\n                    foreach ($s in $skins) { $dict[$s.url] = $s }\n                    $uniqs = @(); foreach ($k in $dict.Keys) { $uniqs += $dict[$k] }\n                    # PowerShell can't emit Python structures easily; rebuild Pythonic list in place is not possible here.\n                    # So we just keep raw parsed 'skins' list; dedup logic will be handled in Python version if needed.\n                    logger.info(\"⚠️ Best-effort parsed items for %s on Divine Skins\", username)\n                    return skins\n'@; (Get-Content $p -Raw).Replace($old, $new) | Set-Content $p -NoNewline; Write-Host 'Skins block patched'",
			"args": [],
			"isBackground": false,
			"problemMatcher": []
		}
	]
}